---
title: äººå·¥æ™ºèƒ½åŸ¹è®­ç¬”è®°ä¹‹äºŒ
tags: äººå·¥æ™ºèƒ½
grammar_cjkRuby: true
---

## 10.25
è¯¾å‰å‡†å¤‡ï¼š
- ç¼–ç¨‹è¯­è¨€ï¼špython3
- IDEï¼šjupyter notebook
- åº“ï¼šsklearnï¼Œpandasï¼Œmatplotlib

ä¸ªäººå»ºè®®ï¼šå®‰è£…ä¸€ä¸ª[anaconda3](https://www.anaconda.com/download/)ï¼Œé™¤äº†sklearn å¯èƒ½éœ€è¦å®‰è£…ï¼Œå…¶å®ƒè¦æ±‚éƒ½è‡ªå¸¦äº†ï¼Œsklearnçš„å®‰è£…åœ¨conda ç»ˆç«¯é‡Œä½¿ç”¨å‘½ä»¤ï¼š

``` python
conda install scikit-learn
```

### åˆ†ç±»ç®—æ³•
#### KNNç®—æ³•
- æ ¸å¿ƒæ€æƒ³ï¼šç‰©ç†ç±»èšï¼Œäººä»¥ç¾¤åˆ†
- æ˜¯æ•°æ®æŒ–æ˜åˆ†ç±»æŠ€æœ¯ä¸­æœ€ç®€å•çš„æ–¹æ³•ä¹‹ä¸€
- æ€è·¯ï¼šå¦‚æœä¸€ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„kä¸ªæœ€ç›¸ä¼¼(å³ç‰¹å¾ç©ºé—´ä¸­æœ€é‚»è¿‘)çš„æ ·æœ¬ä¸­çš„å¤§å¤šæ•°å±äºæŸä¸€ä¸ªç±»åˆ«ï¼Œåˆ™è¯¥æ ·æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«
- è¦ç´ ï¼š
	- è·ç¦»åº¦é‡
		- æ›¼å“ˆé¡¿è·ç¦»
		- æ¬§å¼è·ç¦»
		- å„ä¸ªåæ ‡è·ç¦»æœ€å¤§å€¼
	- Kå€¼é€‰æ‹©
		- è¿‡å°ï¼šæ¨¡å‹å¤æ‚ï¼Œè¿‡æ‹Ÿåˆ
		- è¿‡å¤§ï¼šæ¨¡å‹ç®€å•ï¼Œæ¬ æ‹Ÿåˆ
	- å†³ç­–è§„åˆ™
		- å°‘æ•°æœä»å¤šæ•°

![KNNç¤ºä¾‹](./images/1540435799620.png)

**ä¾‹å­**

``` python
#coding:utf-8

from numpy import *
import operator

##ç»™å‡ºè®­ç»ƒæ•°æ®ä»¥åŠå¯¹åº”çš„ç±»åˆ«
def createDataSet():
    group = array([[1.0,2.0],[1.2,0.1],[0.1,1.4],[0.3,3.5]])
    labels = ['A','A','B','B']
    return group,labels

###é€šè¿‡KNNè¿›è¡Œåˆ†ç±»
def classify(input,dataSe t,label,k):
    dataSize = dataSet.shape[0]
    ####è®¡ç®—æ¬§å¼è·ç¦»
    diff = tile(input,(dataSize,1)) - dataSet
    sqdiff = diff ** 2
    squareDist = sum(sqdiff,axis = 1)###è¡Œå‘é‡åˆ†åˆ«ç›¸åŠ ï¼Œä»è€Œå¾—åˆ°æ–°çš„ä¸€ä¸ªè¡Œå‘é‡
    dist = squareDist ** 0.5
    
    ##å¯¹è·ç¦»è¿›è¡Œæ’åº
    sortedDistIndex = argsort(dist)##argsort()æ ¹æ®å…ƒç´ çš„å€¼ä»å¤§åˆ°å°å¯¹å…ƒç´ è¿›è¡Œæ’åºï¼Œè¿”å›ä¸‹æ ‡

    classCount={}
    for i in range(k):
        voteLabel = label[sortedDistIndex[i]] ###å¯¹é€‰å–çš„Kä¸ªæ ·æœ¬æ‰€å±çš„ç±»åˆ«ä¸ªæ•°è¿›è¡Œç»Ÿè®¡
        classCount[voteLabel] = classCount.get(voteLabel,0) + 1
    ###é€‰å–å‡ºç°çš„ç±»åˆ«æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«
    maxCount = 0
    for key,value in classCount.items():
        if value > maxCount:
            maxCount = value
            classes = key

    return classes
```
#### é€»è¾‘å›å½’
- ä¼˜ç‚¹ï¼š
	- ç®—æ³•ç®€å•ï¼Œè®­ç»ƒé€Ÿåº¦å¿«
	- æ¨¡å‹é²æ£’ï¼Œå·¥ç¨‹ä¸Šç¨³å®šæ€§å¼º
- ç¼ºç‚¹ï¼š
	- éš¾ä»¥å¤„ç†æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜
	- åªèƒ½å¤„ç†çº¿æ€§å¯åˆ†çš„æƒ…å½¢
	- é€‚ç”¨äºäºŒåˆ†ç±»
#### SVM
- ç®—æ³•æ€æƒ³ï¼šé—´éš”æœ€å¤§çš„è¶…å¹³é¢æ¥ä½œä¸ºåˆ†ç±»è¾¹ç•Œ
- ä¼˜ç‚¹ï¼š
	- æœ‰ä¸¥æ ¼æ•°å­¦æ¨ç†
	- é€‚åˆæ•°æ®é‡è¾ƒå°çš„æƒ…å½¢
	- èƒ½å¤Ÿå¤„ç†éçº¿æ€§åˆ†ç±»é—®é¢˜
- ç¼ºç‚¹ï¼š
	-  è®­ç»ƒæ—¶é—´é•¿
	-  åœ¨å¤„ç†å¤šåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¸æ˜¯å¾ˆå¥½

![svmè½¬æ¢è¶…å¹³é¢](./images/1540453908221.png)


### å›å½’é—®é¢˜
>è¿ç»­çš„é¢„æµ‹æ˜¯å›å½’ï¼Œç¦»æ•£æ˜¯åˆ†ç±»ã€‚å¦‚æˆ¿ä»·é¢„æµ‹ï¼Œæ”¶å…¥é¢„æµ‹ã€‚

#### çº¿æ€§å›å½’
- å‡è®¾ç‰¹è¯æ»¡è¶³çº¿æ€§å…³ç³»ï¼Œæ ¹æ®ç»™å®šçš„è®­ç»ƒæ•°æ®è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ç”¨æ­¤æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚ç®€å•æ¥è¯´å°±æ˜¯å·²çŸ¥xï¼Œæ±‚è§£yã€‚
- å®ç°ï¼šåœ¨sklearné‡Œè°ƒç”¨sklearn.linear_model.LinearRegression()
![](./images/1540454441068.png)

#### æ ‘å›å½’
> å½“æ•°æ®æ‹¥æœ‰ä¼—å¤šç‰¹å¾å¹¶ä¸”ç‰¹å¾ä¹‹é—´å…³ç³»ååˆ†å¤æ‚æ—¶ï¼Œæ„å»ºå…¨å±€æ¨¡å‹çš„æƒ³æ³•å°±æ˜¾å¾—å¤ªéš¾äº†ï¼Œä¹Ÿç•¥æ˜¾ç¬¨æ‹™ã€‚è€Œä¸”ï¼Œå®é™…ç”Ÿæ´»ä¸­å¾ˆå¤šé—®é¢˜éƒ½æ˜¯éçº¿æ€§çš„ï¼Œä¸å¯èƒ½ä½¿ç”¨å…¨å±€çº¿æ€§æ¨¡å‹æ¥æ‹Ÿåˆä»»ä½•æ•°æ®ã€‚

- é‡è¦æ€æƒ³ï¼šå±€éƒ¨å›å½’ï¼Œå°†æ•°æ®é›†åˆ†ä¸ºå¤šä»½ï¼Œæ¯ä»½å•ç‹¬å»ºæ¨¡
- ä¸»è¦æ­¥éª¤ï¼šé€’å½’ç”Ÿæˆå›å½’æ ‘ï¼Œå›å½’æ ‘çš„å‰ªæ

**ä¾‹å­**
æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹

``` python
from sklearn.datasets import load_boston
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

# 1 å‡†å¤‡æ•°æ®
# è¯»å–æ³¢å£«é¡¿åœ°åŒºæˆ¿ä»·ä¿¡æ¯
boston = load_boston()
# æŸ¥çœ‹æ•°æ®æè¿°
# print(boston.DESCR)   # å…±506æ¡æ³¢å£«é¡¿åœ°åŒºæˆ¿ä»·ä¿¡æ¯ï¼Œæ¯æ¡13é¡¹æ•°å€¼ç‰¹å¾æè¿°å’Œç›®æ ‡æˆ¿ä»·
# æŸ¥çœ‹æ•°æ®çš„å·®å¼‚æƒ…å†µ
# print("æœ€å¤§æˆ¿ä»·ï¼š", np.max(boston.target))   # 50
# print("æœ€å°æˆ¿ä»·ï¼š",np.min(boston.target))    # 5
# print("å¹³å‡æˆ¿ä»·ï¼š", np.mean(boston.target))   # 22.532806324110677

x = boston.data
y = boston.target

# 2 åˆ†å‰²è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
# éšæœºé‡‡æ ·25%ä½œä¸ºæµ‹è¯• 75%ä½œä¸ºè®­ç»ƒ
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=33)


# 3 è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
ss_x = StandardScaler()
x_train = ss_x.fit_transform(x_train)
x_test = ss_x.transform(x_test)

ss_y = StandardScaler()
y_train = ss_y.fit_transform(y_train.reshape(-1, 1))
y_test = ss_y.transform(y_test.reshape(-1, 1))

# 4 ä½¿ç”¨å›å½’æ ‘è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹
# åˆå§‹åŒ–kè¿‘é‚»å›å½’æ¨¡å‹ ä½¿ç”¨å¹³å‡å›å½’è¿›è¡Œé¢„æµ‹
dtr = DecisionTreeRegressor()
# è®­ç»ƒ
dtr.fit(x_train, y_train)
# é¢„æµ‹ ä¿å­˜é¢„æµ‹ç»“æœ
dtr_y_predict = dtr.predict(x_test)

# 5 æ¨¡å‹è¯„ä¼°
print("å›å½’æ ‘çš„é»˜è®¤è¯„ä¼°å€¼ä¸ºï¼š", dtr.score(x_test, y_test))
print("å¹³å›å½’æ ‘çš„R_squaredå€¼ä¸ºï¼š", r2_score(y_test, dtr_y_predict))
print("å›å½’æ ‘çš„å‡æ–¹è¯¯å·®ä¸º:", mean_squared_error(ss_y.inverse_transform(y_test),
                                           ss_y.inverse_transform(dtr_y_predict)))
print("å›å½’æ ‘çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º:", mean_absolute_error(ss_y.inverse_transform(y_test),
                                               ss_y.inverse_transform(dtr_y_predict)))

'''
å›å½’æ ‘çš„é»˜è®¤è¯„ä¼°å€¼ä¸ºï¼š 0.7066505912533438
å¹³å›å½’æ ‘çš„R_squaredå€¼ä¸ºï¼š 0.7066505912533438
å›å½’æ ‘çš„å‡æ–¹è¯¯å·®ä¸º: 22.746692913385836
å›å½’æ ‘çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º: 3.08740157480315
'''
```

#### GBRTï¼ˆæ¸è¿›æ¢¯åº¦å›å½’æ ‘ï¼‰
- æ˜¯ä¸€ä¸ªboostingï¼Œå…¶ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¸€å †å¼±åˆ†ç±»å™¨çš„ç»„åˆå°±å¯ä»¥æˆä¸ºä¸€ä¸ªå¼ºåˆ†ç±»å™¨ï¼ŒåŒæ—¶ä¸æ–­åœ°åœ¨é”™è¯¯ä¸­å­¦ä¹ ï¼Œè¿­ä»£æ¥é™ä½çŠ¯é”™æ¦‚ç‡ã€‚å³ï¼Œæ¯ä¸€è½®åŸºå­¦ä¹ å™¨è®­ç»ƒè¿‡åéƒ½ä¼šæ›´æ–°æ ·æœ¬æƒé‡ï¼Œå†è®­ç»ƒä¸‹ä¸€ä¸ªå­¦ä¹ å™¨ï¼Œæœ€åå°†æ‰€æœ‰çš„åŸºå­¦ä¹ å™¨åŠ æƒç»„åˆã€‚
- Gradient Boosting åœ¨è¿­ä»£çš„æ—¶å€™é€‰æ‹©æ¢¯åº¦ä¸‹é™çš„æ–¹å‘æ¥ä¿è¯æœ€åçš„ç»“æœæœ€å¥½ã€‚

**ä¾‹å­**
æ•°æ®è¿˜æ˜¯æ³¢å£«é¡¿æˆ¿ä»·

``` python
import numpy as np
import matplotlib.pyplot as plt

from sklearn import ensemble  
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error

# 1 å‡†å¤‡æ•°æ®
# è¯»å–æ³¢å£«é¡¿åœ°åŒºæˆ¿ä»·ä¿¡æ¯ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œæ‰“ä¹±ï¼Œåˆ†å‰²æµ‹è¯•é›†ä¸è®­ç»ƒé›†
boston = datasets.load_boston()
X, y = shuffle(boston.data, boston.target, random_state=13)
X = X.astype(np.float32)
offset = int(X.shape[0] * 0.9)
X_train, y_train = X[:offset], y[:offset]
X_test, y_test = X[offset:], y[offset:]

# 2.è®­ç»ƒå›å½’æ¨¡å‹
params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,
          'learning_rate': 0.01, 'loss': 'ls'}
clf = ensemble.GradientBoostingRegressor(**params)

clf.fit(X_train, y_train)
mse = mean_squared_error(y_test, clf.predict(X_test))
print("MSE: %.4f" % mse)

# 3.ç”»å‡ºè®­ç»ƒåå·®ï¼Œè®¡ç®—æµ‹è¯•é›†åå·®
test_score = np.zeros((params['n_estimators'],), dtype=np.float64)

for i, y_pred in enumerate(clf.staged_predict(X_test)):
    test_score[i] = clf.loss_(y_test, y_pred)

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title('Deviance')
plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',
         label='Training Set Deviance')
plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',
         label='Test Set Deviance')
plt.legend(loc='upper right')
plt.xlabel('Boosting Iterations')
plt.ylabel('Deviance')

# ç”»å‡ºç‰¹å¾çš„é‡è¦æ€§ï¼ˆpsï¼šè¿™ä¸ªå¾ˆå¥½ï¼Œå…·æœ‰å¯è§£é‡Šæ€§ï¼Œä¸åŒäºç¥ç»ç½‘ç»œçš„é»‘ç›’ï¼‰
feature_importance = clf.feature_importances_
feature_importance = 100.0 * (feature_importance / feature_importance.max())
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
plt.subplot(1, 2, 2)
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, boston.feature_names[sorted_idx])
plt.xlabel('Relative Importance')
plt.title('Variable Importance')
plt.show()
```
![Gradient Boostingç»“æœ](./images/1540439152084.png)

### èšç±»ç®—æ³•

#### k-means
- é€‚ç”¨äºçƒçŠ¶åˆ†å¸ƒ
- æ ¸å¿ƒé—®é¢˜ï¼šåˆå§‹ç‚¹çš„é€‰å–ï¼Œ ğ‘˜å€¼çš„é€‰å–ï¼Œè·ç¦»å…¬å¼
- ä¼˜ç‚¹ï¼š
	- é€‚ç”¨æ€§å¼º
	- æ”¶æ•›é€Ÿåº¦å¿«
	- èšç±»ç»“æœå±€éƒ¨æœ€ä¼˜
- ç¼ºç‚¹ï¼šä¾èµ–å‚æ•°çš„é€‰å–

**ä¾‹å­**
1.é¦–å…ˆç”Ÿæˆæ•°æ®
``` python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.datasets.samples_generator import make_blobs
# Xä¸ºæ ·æœ¬ç‰¹å¾ï¼ŒYä¸ºæ ·æœ¬ç°‡ç±»åˆ«ï¼Œ å…±1000ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬4ä¸ªç‰¹å¾ï¼Œå…±4ä¸ªç°‡ï¼Œç°‡ä¸­å¿ƒåœ¨[-1,-1], [0,0],[1,1], [2,2]ï¼Œ ç°‡æ–¹å·®åˆ†åˆ«ä¸º[0.4, 0.2, 0.2]
X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1,-1], [0,0], [1,1], [2,2]], cluster_std=[0.4, 0.2, 0.2, 0.2], random_state =9)
plt.scatter(X[:, 0], X[:, 1], marker='o')
plt.show()
```
![ç”Ÿæˆæ•°æ®å›¾](./images/1540472501227.png)
çœ‹ä»¥ä¸Šæ•°æ®ï¼Œk=4æ¯”è¾ƒåˆç†ï¼Œæˆ‘ä»¬å¯ä»¥è®©k = [2,5] ï¼Œç„¶åæŠŠä½¿ç”¨Calinski-Harabasz Indexè¯„ä¼°çš„èšç±»åˆ†æ•°ã€‚

``` python
from sklearn.cluster import KMeans
for index, k in enumerate((2,3,4,5)):
	plt.subplot(2,2,index+1)
	y_pred = KMeans(n_clusters=k, random_state=9).fit_predict(X)
	plt.scatter(X[:, 0], X[:, 1], c=y_pred)
	metrics.calinski_harabaz_score(X, y_pred) 
    plt.text(.99, .01, ('k=%d, score: %.2f' % (k,score)),
                 transform=plt.gca().transAxes, size=10,
                 horizontalalignment='right')
plt.show()
```
![ä¸åŒkå€¼çš„èšç±»ç»“æœ](./images/1540472875574.png)


#### DBSCAN
- å…³é”®å‚æ•°ï¼šepsï¼ˆç»™å®šå¯¹è±¡åŠå¾„ï¼‰ï¼Œmin_samplesï¼ˆé‚»åŸŸè‡³å°‘åŒ…å«MinPtsä¸ªæ ·æœ¬ï¼‰
- ä¼˜ç‚¹ï¼š
	- å¾ˆå¥½çš„æŠ‘åˆ¶å™ªå£°
	- è§£å†³éçƒçŠ¶ç»“æ„çš„èšç±»æ•°æ®
	- ä¸éœ€è¦æŒ‡å®šèšç±»çš„ä¸ªæ•°
- ç¼ºç‚¹ï¼š
	- å¯¹ç”¨æˆ·å®šä¹‰çš„å‚æ•°æ•æ„Ÿï¼ˆå¦‚MinPts ï¼ŒEpsï¼‰
	- å…¨å±€å¯†åº¦å‚æ•°ä¸èƒ½åˆ»ç”»å†…åœ¨çš„èšç±»ç»“æ„


**ä¾‹å­**
1.é¦–å…ˆç”Ÿæˆæ•°æ®
``` python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X1, y1=datasets.make_circles(n_samples=5000, factor=.6,
                                      noise=.05)
X2, y2 = datasets.make_blobs(n_samples=1000, n_features=2, centers=[[1.2,1.2]], cluster_std=[[.1]],
               random_state=9)

X = np.concatenate((X1, X2))
plt.scatter(X[:, 0], X[:, 1], marker='o')
plt.show()
```
![ç”Ÿæˆçš„æ•°æ®å›¾](./images/1540473009701.png)

å¦‚æœä½¿ç”¨k-meansæ¥èšç±»ï¼Œæ•ˆæœå¾ˆä¸å¥½ï¼Œä»¤k=3ï¼Œèšç±»æ•ˆæœå¦‚ä¸‹

![k-meansèšç±»ç»“æœå›¾](./images/1540473087918.png)
ä½¿ç”¨DBSCANèšç±»

``` python
from sklearn.cluster import DBSCAN
y_pred = DBSCAN(eps = 0.1, min_samples = 10).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```
![DBSCANèšç±»ç»“æœ](./images/1540473445386.png)

### æ¨èç®—æ³•
#### ååŒè¿‡æ»¤
- user-basedï¼Œ item-based
- ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼šä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ¬§æ°è·ç¦»ï¼Œçš®å°”é€Šç›¸å…³ç³»æ•°

#### çŸ©é˜µåˆ†è§£
https://my.oschina.net/keyven/blog/513850
https://blog.csdn.net/qq_26225295/article/details/51165858
- user-itemçŸ©é˜µ

![ç”¨æˆ·-ç‰©å“çš„è¯„åˆ†çŸ©é˜µ](./images/1540473573792.png)

### æ¡ˆä¾‹åˆ†äº«
å›¢é˜Ÿä¿é™©çš„å®šä»·
ä¸šåŠ¡é€»è¾‘ï¼š
è¯¢ä»·--å®šä»·--è°ƒçœ‹sqlé‡Œçš„æ•°æ®ï¼ˆäº§å“ä¿¡æ¯ï¼Œä¿å•ä¿¡æ¯ï¼Œå‡ºé™©ä¿¡æ¯ï¼‰

å°†è¿ç»­ç‰¹å¾å½’ä¸€åŒ–
ä¿é¢ï¼Œäººæ•°ï¼Œç”·æ€§å æ¯”

#### å›¢é™©å®šä»·æ¨¡å‹
ç™¾åº¦å¼€å‘çš„DNNæ¨¡å‹ï¼ŒåŸºäºkeraså¼€å‘
æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼š
- MAEä¸è¯¯å·®ç»å¯¹å€¼å‘ˆçº¿æ€§å…³ç³»
- MSEæƒ©ç½šè¯¯å·®è¾ƒå¤§çš„æ ·æœ¬
- MSLEé€‚åˆé¢„æµ‹å€¼èŒƒå›´å¤§çš„æ ·æœ¬

## 10.26
è®²å¸ˆï¼šç™¾åº¦é«˜çº§ç ”å‘å·¥ç¨‹å¸ˆ

### æ·±åº¦å­¦ä¹ åŸºç¡€
**å†³ç­–æ¨¡å‹**
ä»¥è¯„åˆ¤ä¸€ä¸ªäººæ˜¯å¦æ˜¯ä¸€ä¸ªä¸‰å¥½å­¦ç”Ÿä¸ºä¾‹ï¼Œå‡è®¾æœ‰ä¸‰é—¨è¯¾çš„æˆç»©ï¼Œä¸‰é—¨æˆç»©åˆ†åˆ«æœ‰ä¸åŒçš„æƒå€¼ï¼ŒåŠ æƒæˆç»©å¤§äº90çš„æ˜¯ä¸‰å¥½å­¦ç”Ÿã€‚

sigmoidæ¿€æ´»å‡½æ•°
	- éçº¿æ€§
	- å¤„å¤„å¯å¾®
	- éé¥±å’Œ
	- å•è°ƒé€’å¢

æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µ
https://blog.csdn.net/tsyccnh/article/details/79163834

**æ¨¡å‹è®­ç»ƒçš„æ–¹æ³•**
æ¨¡å‹æ ¹æ®æ ·æœ¬æ•°æ®ï¼Œæ‰¾å‡ºæƒå€¼ï¼Œç„¶åæ ¹æ®è°ƒæ•´å¥½çš„æƒå€¼é¢„æµ‹æ–°æ ·æœ¬çš„lableã€‚
softmaxçš„ä½œç”¨æ˜¯åœ¨ç½‘ç»œè¾“å‡ºç«¯è¿›è¡Œå½’ä¸€åŒ–ï¼ˆå½“å¤„ç†å¤šåˆ†ç±»çš„ä»»åŠ¡æ—¶ï¼‰ï¼Œä½¿è¾“å‡ºçš„å‘é‡ä¹‹å’Œä¸º1ï¼Œå‘é‡ä¸­maxå¯¹åº”çš„æ ‡ç­¾åˆ™æ˜¯é¢„æµ‹çš„æ ‡ç­¾ã€‚

[åå‘ä¼ æ’­](https://www.cnblogs.com/charlotte77/p/5629865.html)çš„ä½œç”¨ï¼šæ ¹æ®æœ€åè¾“å…¥å’Œå®é™…æƒ…å†µçš„è¯¯å·®ï¼Œåå‘ä¼ æ’­åˆ°å‰é¢çš„ç¥ç»å±‚ï¼Œè®©å‰é¢ç¥ç»å±‚çš„æƒé‡æ€ä¹ˆæ ¹æ®è¯¯å·®è¿›è¡Œè°ƒæ•´ï¼Œæ—¨åœ¨å¾—åˆ°æœ€ä¼˜çš„å…¨å±€å‚æ•°çŸ©é˜µã€‚

å‡½æ•°å•†çš„æ±‚å¯¼æ³•åˆ™ï¼š[f(x)/g(x)]'=[f'(x)g(x)-f(x)g'(x)]/[g(x)]^2


åŒå‘RNNï¼ŒtçŠ¶æ€ä¸ä»…å’Œå‰é¢çš„t-1æ—¶åˆ»æœ‰å…³ï¼Œä¹Ÿå’Œt+1æ—¶åˆ»æœ‰å…³

LSTMå’ŒGRUçš„åŒºåˆ«ï¼š
- GRU2ä¸ªé—¨ï¼ŒLSTM3ä¸ªé—¨
- GRUæ— çŠ¶æ€C
- æ§åˆ¶æ–¹å¼

### åºåˆ—å»ºæ¨¡æ¡ˆä¾‹
#### æˆä¿¡å®¡æ‰¹æ„è§æŠ½å–
- æ•°æ®åˆ†æï¼Œå­—æ®µæ˜¯å¦é€‚ç”¨æœºå™¨å­¦ä¹ çš„æ–¹æ³•æ¥æŠ½å–
	- æ ·æœ¬æ˜¯å¦å……è¶³
	- æ˜¯å¦åœ¨åŸå§‹æ–‡æœ¬ä¸­æœ‰æ ‡æ³¨
- ç¨€ç–è¡¨ç¤ºï¼Œåˆ†å¸ƒå¼è¡¨ç¤º---->è¿›ä¸€æ­¥æŠ½è±¡------>å­—ç²’åº¦ï¼Œè¯ç²’åº¦
- ç°åœ¨çš„åˆ†å¸ƒå¼è¡¨ç¤ºå–œæ¬¢å°†è¯åµŒå…¥+å¥åµŒå…¥+è°“è¯­åµŒå…¥
- å®é™…é—®é¢˜
	- è¯†åˆ«è„±æ•å®ä½“ï¼Œæƒ³æ³•ï¼šå®ä½“æ˜¯ä¸€ä¸ªè¯ï¼Œæ ‡æ³¨å®ä½“å‰åçš„è¯ï¼Œæ ‡æ³¨å­—
- æ¨¡å‹è®¾è®¡
	- word embeddingï¼Œbi-lstm encoderï¼Œcrf layer
	- åå¤„ç†ï¼šviterbiè§£ç ï¼Œä½œç”¨æ˜¯é™ä½è®¡ç®—å¤æ‚åº¦
	- attentionæœºåˆ¶
	- HAN
	- self-attentionï¼Œå¯»æ‰¾åºåˆ—å†…éƒ¨çš„è”ç³»
	- multi-head attention
	- transformer
	- bertï¼Œtransformerçš„å†å°è£…
![HANç½‘ç»œç»“æ„](./images/1540525657991.png)


#### æå–å…³é”®è¯
ä½¿ç”¨attentionçš„æ–¹æ³•æ¥åšï¼Œå¼•å…¥äº†ä¸€ä¸ªå¤–éƒ¨å˜é‡æ¥ä»£è¡¨å…¨æ–‡çš„ä¿¡æ¯

### äººå·¥æ™ºèƒ½å¹³å°
#### ç®€ä»‹
- ç ”å‘æµç¨‹ï¼šæ•°æ®å¤„ç†---ç‰¹å¾å·¥ç¨‹---æ¨¡å‹è®­ç»ƒ---æ¨¡å‹è¯„ä¼°----æ¨¡å‹é¢„æµ‹
- æ·±åº¦å­¦ä¹ æ¡†æ¶å‘å±•è¿‡ç¨‹ï¼ˆå‘èµ·äºå­¦æœ¯ç•Œï¼Œé€æ¸æ¼”å˜ä¸ºå·¨å¤´ç«äº‰ï¼‰
	- Theano ï¼ˆ2010ï¼‰
	- Caffeï¼ˆ2013ï¼‰
	- Tensorflowï¼ˆ2015ï¼‰
	- MXNetï¼ˆ2015ï¼‰
	- PaddlePaddleï¼ˆ2016ï¼‰
	- PyTorchï¼ˆ2017ï¼‰
	- Caffe2ï¼ˆ2017ï¼‰
#### Tensorflow
é‡‡ç”¨dataflow graph æ¨¡å‹è¡¨ç¤ºè¿ç®—å…³ç³»ï¼Œå†å°†è¿ç®—æ˜ å°„åˆ°ä¸åŒçš„ç¡¬ä»¶è¿›è¡Œæ‰§è¡Œã€‚

**ä½çº§api**
tensorï¼šå˜é‡ï¼Œå¸¸é‡ï¼Œå ä½ï¼Œç¨€ç–
operatorï¼šä»¥tensorä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºçš„è¿ç®—å•å…ƒ
graphï¼šä»¥operatorä¸ºèŠ‚ç‚¹ï¼Œä»¥tensorsè¾“å…¥æˆ–è€…è¾“å‡ºä¸ºè¾¹
sessionï¼šå°è£…è¿è¡Œæ—¶çŠ¶æ€ï¼Œè¿è¡ŒæŒ‡å®šçš„operatorï¼Œå ç”¨å†…å­˜ç”¨äºå­˜å‚¨èµ„æºå¯¹è±¡ï¼Œè°ƒç”¨closeæ–¹æ³•åé‡Šæ”¾
tf.deviceï¼šå·¥å…·å‡½æ•°ï¼ŒæŒ‡å®šä¸€ç»„è¿ç®—æ“ä½œåœ¨ç‰¹å®šçš„è®¾å¤‡ä¸Šè¿›è¡Œ
**ä¸­çº§api**
layerï¼šå°è£…å˜é‡å’Œå˜é‡ä¸Šçš„è¿ç®—æ“ä½œï¼Œå¦‚cnnä¸­çš„å·ç§¯å±‚ï¼Œæ± åŒ–å±‚ï¼Œflattenï¼šå‹å¹³è¾“å…¥tensorå±‚ï¼Œå¦‚æŠŠä¸€ä¸ª2ç»´è¾“å…¥å˜æ¢æˆ1ç»´
dataï¼šæä¾›è¾“å…¥æ•°æ®å¤„ç†pipelineçš„apiç±»åŒ…
metricsï¼šå°è£…è¯„ä¼°è¿ç®—ç›¸å…³çš„apiç±»åŒ…
**é«˜çº§api**
estimators ï¼Œæ•ˆç‡æœ€é«˜ï¼Œæä¾›æ¨¡å‹å°è£…ï¼Œå¦‚DNNåˆ†ç±»å™¨ï¼Œçº¿æ€§åˆ†ç±»å™¨
- è®­ç»ƒtrain
- è¯„ä¼°evaluate
- é¢„æµ‹predict
- å¯¼å‡ºæ¨¡å‹export_savemodel

**åˆ†å¸ƒå¼api**
ä¸€éƒ¨åˆ†è´Ÿè´£å‚æ•°æ›´æ–°ç»´æŠ¤ï¼Œä¸€éƒ¨åˆ†è´Ÿè´£æ ·æœ¬è®¡ç®—ä»»åŠ¡ï¼Œéš¾é¢˜ï¼šworkerå’Œparameter serverçš„é€šä¿¡
- clusterï¼Œè®¡ç®—é›†ç¾¤ï¼Œç”±ä¸€ç¾¤jobç»„æˆ
- jobï¼Œä¸€ç»„ç›¸åŒçš„ä»»åŠ¡ï¼Œå¦‚worker
- taskï¼Œå…·ä½“æ‰§è¡Œè®¡ç®—ä»»åŠ¡å¯¹çš„æœåŠ¡èŠ‚ç‚¹

å¹¶è¡Œæ–¹æ¡ˆï¼š
- æ•°æ®å¹¶è¡Œï¼šèŠ‚ç‚¹å¹¶è¡Œè®¡ç®—å„æ‰¹æ¬¡æ ·æœ¬æ•°æ®
- æ¨¡å‹å¹¶è¡Œï¼šæ¨¡å‹ç»“æ„æ‹†åˆ†è‡³å¤šèŠ‚ç‚¹è®¡ç®—

**Tensorflowæ¶æ„**
https://www.jianshu.com/p/a5574ebcdeab
![enter description here](./images/1540540755072.png)

**å¯è§†åŒ–å·¥å…· TensorBoard**
èƒ½å¤Ÿæœ‰æ•ˆåœ°å±•ç¤ºTensorflowåœ¨è¿è¡Œè¿‡ç¨‹ä¸­çš„è®¡ç®—å›¾ã€å„ç§æŒ‡æ ‡éšç€æ—¶é—´çš„å˜åŒ–è¶‹åŠ¿ä»¥åŠè®­ç»ƒä¸­ä½¿ç”¨åˆ°çš„æ•°æ®ä¿¡æ¯ã€‚
å…·ä½“ä½¿ç”¨æ–¹æ³•ï¼šhttps://github.com/tensorflow/tensorboard/blob/master/README.md

``` python
from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

batch_size = 100
hidden1_nodes = 200
with tf.name_scope('Input'):  #å›¾èŠ‚ç‚¹åç§°
    x = tf.placeholder(tf.float32,shape=(None,784))
    y = tf.placeholder(tf.float32,shape=(None,10))
with tf.name_scope('Inference'):
    w1 = tf.Variable(tf.random_normal([784,hidden1_nodes],stddev=0.1))
    w2 = tf.Variable(tf.random_normal([hidden1_nodes,10],stddev=0.1))
    b1 = tf.Variable(tf.random_normal([hidden1_nodes],stddev=0.1))
    b2 = tf.Variable(tf.random_normal([10],stddev=0.1))
    hidden = tf.nn.relu(tf.matmul(x,w1)+b1)
    y_predict = tf.nn.relu(tf.matmul(hidden,w2)+b2)

with tf.name_scope('Loss'):
    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_predict))
with tf.name_scope('Train'):
    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
with tf.name_scope('Accuracy'):
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_predict, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(10000):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})
        if i%1000==0:
            print ('Phase'+str(i/1000+1)+':',sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))
writer = tf.summary.FileWriter("./mnist_nn_log",sess.graph)
writer.close()
```
ç”Ÿæˆçš„ç»“æ„å›¾
![enter description here](./images/1540541310431.png)

**æ¨¡å‹é¢„æµ‹æœåŠ¡ Tensorflow Serving**
æ¨¡å‹æ–‡ä»¶ï¼šå›¾ç»“æ„ï¼Œå‚æ•°å€¼ï¼Œè¾“å…¥ï¼Œè¾“å‡º
- ååºåˆ—åŒ–æ–‡ä»¶æ¨¡å‹
- åŠ è½½æ¨¡å‹
- è½¬å‘ä¸­æ¢ï¼Œå†³å®šè¯·æ±‚æµé‡çš„åˆ†é…ï¼Œè¿˜æœ‰ç›‘å¬ä½œç”¨

#### Kerasï¼ˆå¤ªé«˜çº§ï¼‰
ç°åœ¨å’ŒTensorFlowäº’ç›¸èåˆäº†ï¼Œtf.keras
**ä¾‹å­**
æ•°æ®é›†ï¼šFashion-MNIST
https://blog.csdn.net/LuohenYJ/article/details/81091641?utm_source=blogxgwz3

#### PaddlePaddle


## ç™¾åº¦ä¸€ä½“æœºä»‹ç»ä»¥åŠåº”ç”¨å®è·µ
ä¸»è®²äººï¼šå¾ä¸œæ³½

dockerçš„å¥½å¤„
- ä¸å®¿ä¸»æœºå…±äº«å†…æ ¸ï¼ŒèŠ‚çº¦èµ„æº
- å°è£…æ€§
- éš”ç¦»æ€§
- é•œåƒå¢é‡åˆ†å‘ï¼ŒèŠ‚çº¦å¸¦å®½
- ç¤¾åŒºæ´»è·ƒ

dockerçš„åå¤„
- å…±ç”¨å†…æ ¸å¯¼è‡´å­˜åœ¨ç©¿é€é—®é¢˜ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£

æŒ‚è½½çš„æ•°æ®å¯ä»¥å®æ—¶æ›´æ–°

deploymentç»§æ‰¿äº†RCçš„å…¨éƒ¨åŠŸèƒ½ï¼Œè¿˜å¢åŠ ä»¥ä¸‹ï¼š
- äº‹ä»¶å’ŒçŠ¶æ€æŸ¥çœ‹
- å›æ»šä¸æ»šåŠ¨å‡çº§

flannelç½‘ç»œ

![enter description here](./images/1540607529028.png)




